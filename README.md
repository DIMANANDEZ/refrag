# README.md

# REFRAG: Representation-Focused Retrieval Augmented Generation

Early open-source implementation of representation-optimized RAG for better retrieval quality.

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

## üöÄ What is REFRAG?

Traditional RAG systems embed and retrieve raw document chunks. **REFRAG** uses an LLM to generate optimized representations of each chunk before embedding, resulting in:

- **Better retrieval quality**: LLM-powered representations focus on key information
- **Smaller context windows**: Condensed representations = fewer tokens to LLM
- **Improved relevance**: Semantic understanding > pure vector similarity

Based on emerging research in representation-focused retrieval. This is an early practical implementation.

## ‚ö° Quick Start

### Installation

```bash
pip install refrag

# Or with specific LLM provider
pip install "refrag[openai]"
pip install "refrag[anthropic]"
pip install "refrag[all]"  # Both providers
```

### Basic Usage

```python
from refrag import REFRAGRetriever

# Initialize
retriever = REFRAGRetriever(
    llm_provider="openai",  # or "anthropic"
    llm_model="gpt-4o-mini",
    embedding_model="sentence-transformers/all-MiniLM-L6-v2"
)

# Index your documents
documents = [
    "Python is a programming language...",
    "Machine learning involves...",
    # ... more documents
]

retriever.index(documents, show_progress=True)

# Retrieve relevant chunks
results = retriever.retrieve(
    query="Tell me about programming languages",
    top_k=5,
    return_scores=True
)

for result in results:
    print(f"Score: {result['score']}")
    print(f"Text: {result['text']}")
    print(f"Representation: {result['representation']}\n")
```

## üìä How It Works

```
Traditional RAG:
Document Chunk ‚Üí Embedding ‚Üí Vector DB ‚Üí Retrieve

REFRAG:
Document Chunk ‚Üí LLM Representation ‚Üí Embedding ‚Üí Vector DB ‚Üí Retrieve
                 ‚Üë
                 This is the key difference
```

**Example:**

```python
# Original chunk:
"The company was founded in 1998 in Seattle by Jeff Bezos. It started as an online bookstore but expanded into cloud computing, digital streaming, and artificial intelligence."

# REFRAG representation (generated by LLM):
"Amazon: Founded 1998 by Jeff Bezos in Seattle. Evolved from online bookstore to tech giant with cloud (AWS), streaming, and AI services."
```

The representation is:

- ‚úÖ More concise
- ‚úÖ Focused on key facts
- ‚úÖ Better for semantic matching

## üéØ Use Cases

- **Research papers**: Extract key findings instead of embedding full paragraphs
- **Documentation**: Focus on actionable information
- **Long-form content**: Condense while preserving meaning
- **Multi-lingual**: LLM can normalize to single language

## üîß Advanced Usage

### Custom Representation Prompt

```python
from refrag import REFRAGEmbedder

custom_prompt = """
Summarize this technical documentation chunk:
- Focus on API endpoints and parameters
- Include code examples if present
- Maximum 3 sentences

Text: {chunk}

Summary:
"""

embedder = REFRAGEmbedder(
    representation_prompt=custom_prompt
)

retriever = REFRAGRetriever(embedder=embedder)
```

### With Reranking

```python
from refrag import REFRAGRetriever, REFRAGReranker

retriever = REFRAGRetriever()
retriever.index(documents)

# Initial retrieval
results = retriever.retrieve(query, top_k=10)

# Rerank top results
reranker = REFRAGReranker(llm_provider="openai")
final_results = reranker.rerank(query, results, top_k=5)
```

## üìä Benchmarks

| Method      | Top Result                         | Retrieval Time | Index Time (5 docs) |
| ----------- | ---------------------------------- | -------------- | ------------------- |
| Vanilla RAG | Python, Rust, **JavaScript**       | 0.168s         | 0.33s               |
| REFRAG      | Python, Rust, **Machine Learning** | **0.029s**     | 7.4s                |

Query: "What programming languages are good for AI development?"

**REFRAG correctly ranked Machine Learning content over JavaScript** - better semantic understanding through LLM representations.

[See full comparison](examples/compare_with_vanilla_rag.py)

## üõ†Ô∏è Development

```bash
# Clone repo
git clone https://github.com/yourusername/refrag.git
cd refrag

# Install in development mode
pip install -e ".[dev]"

# Run tests
pytest

# Run examples
python examples/basic_usage.py
python examples/compare_with_vanilla_rag.py
```

## üìù Requirements

- Python 3.8+
- OpenAI API key (for `llm_provider="openai"`) or Anthropic API key (for `llm_provider="anthropic"`)

Set your API key:

```bash
export OPENAI_API_KEY="your-key-here"
# or
export ANTHROPIC_API_KEY="your-key-here"
```

## üó∫Ô∏è Roadmap

- [ ] Comprehensive benchmarks on standard datasets
- [ ] Support for local LLMs (Ollama, etc.)
- [ ] Async/batch processing optimizations
- [ ] Integration with LangChain/LlamaIndex
- [ ] Pre-generated representation caching
- [ ] Multi-modal support (images, tables)

## ü§ù Contributing

Early stage - contributions welcome! Areas of interest:

- Benchmark datasets and evaluation
- Additional LLM provider support
- Performance optimizations
- Documentation improvements

## üìÑ License

MIT License - see [LICENSE](LICENSE) file.

## üôè Acknowledgments

Inspired by research in representation learning for retrieval systems. This is an independent implementation for the open-source community.

## ‚ö†Ô∏è Status

**Early Alpha** - API may change. Use in production at your own risk. Feedback welcome!

---

**Built by [@shaiv](https://medium.com/@shaiv) | [GovernsAI](https://yourcompanyurl.com)**
